{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re as re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>publicationTime</th>\n",
       "      <th>bodyText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id:twitter.com:2209799083</td>\n",
       "      <td>10/02/2016 10:30</td>\n",
       "      <td>RT @350Australia: Adani Group's Aust #coal min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id:twitter.com:2209799083</td>\n",
       "      <td>10/02/2016 10:36</td>\n",
       "      <td>RT @avivaimhof: Poor old #coal. Now even #Viet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>id:twitter.com:43060113</td>\n",
       "      <td>10/02/2016 10:37</td>\n",
       "      <td>RT @market_forces: Funds have burned billions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>id:twitter.com:2209799083</td>\n",
       "      <td>10/02/2016 10:37</td>\n",
       "      <td>RT @avivaimhof: #Vietnam PM Announces Retreat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>id:twitter.com:585560584</td>\n",
       "      <td>10/02/2016 10:41</td>\n",
       "      <td>RT @350Australia: Adani Group's Aust #coal min...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                     author   publicationTime  \\\n",
       "0          0  id:twitter.com:2209799083  10/02/2016 10:30   \n",
       "1          1  id:twitter.com:2209799083  10/02/2016 10:36   \n",
       "2          2    id:twitter.com:43060113  10/02/2016 10:37   \n",
       "3          3  id:twitter.com:2209799083  10/02/2016 10:37   \n",
       "4          4   id:twitter.com:585560584  10/02/2016 10:41   \n",
       "\n",
       "                                            bodyText  \n",
       "0  RT @350Australia: Adani Group's Aust #coal min...  \n",
       "1  RT @avivaimhof: Poor old #coal. Now even #Viet...  \n",
       "2  RT @market_forces: Funds have burned billions ...  \n",
       "3  RT @avivaimhof: #Vietnam PM Announces Retreat ...  \n",
       "4  RT @350Australia: Adani Group's Aust #coal min...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('realFinalNHB.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate tweets into dates\n",
    "data['publicationTime'] = data['publicationTime'].apply(lambda x : pd.to_datetime(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['publicationTime'] = data['publicationTime'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.date(2016, 10, 2), datetime.date(2016, 11, 2),\n",
       "       datetime.date(2016, 12, 2), datetime.date(2016, 2, 13),\n",
       "       datetime.date(2016, 2, 14), datetime.date(2016, 2, 15),\n",
       "       datetime.date(2016, 2, 16), datetime.date(2016, 2, 17),\n",
       "       datetime.date(2016, 2, 18), datetime.date(2016, 2, 19),\n",
       "       datetime.date(2016, 2, 20), datetime.date(2016, 2, 21),\n",
       "       datetime.date(2016, 2, 22), datetime.date(2016, 2, 23),\n",
       "       datetime.date(2016, 2, 24), datetime.date(2016, 2, 25),\n",
       "       datetime.date(2016, 2, 26), datetime.date(2016, 2, 27),\n",
       "       datetime.date(2016, 2, 28), datetime.date(2016, 2, 29),\n",
       "       datetime.date(2016, 1, 3), datetime.date(2016, 2, 3),\n",
       "       datetime.date(2016, 3, 3), datetime.date(2016, 4, 3),\n",
       "       datetime.date(2016, 5, 3), datetime.date(2016, 6, 3),\n",
       "       datetime.date(2016, 7, 3), datetime.date(2016, 8, 3), nan,\n",
       "       datetime.date(2016, 9, 3), datetime.date(2016, 10, 3),\n",
       "       datetime.date(2016, 11, 3), datetime.date(2016, 3, 15),\n",
       "       datetime.date(2016, 3, 16)], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['publicationTime'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['publicationTime'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13394 entries, 0 to 13393\n",
      "Data columns (total 4 columns):\n",
      "Unnamed: 0         13378 non-null object\n",
      "author             13373 non-null object\n",
      "publicationTime    13373 non-null datetime64[ns]\n",
      "bodyText           13373 non-null object\n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 418.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "publicationTime\n",
       "2016-01-03    310\n",
       "2016-02-03    410\n",
       "2016-02-13     41\n",
       "2016-02-14     31\n",
       "2016-02-15    441\n",
       "2016-02-16    490\n",
       "2016-02-17    468\n",
       "2016-02-18    420\n",
       "2016-02-19    916\n",
       "2016-02-20    398\n",
       "2016-02-21    510\n",
       "2016-02-22    528\n",
       "2016-02-23    504\n",
       "2016-02-24    191\n",
       "2016-02-25    283\n",
       "2016-02-26    495\n",
       "2016-02-27    548\n",
       "2016-02-28    277\n",
       "2016-02-29    134\n",
       "2016-03-03    439\n",
       "2016-03-15    317\n",
       "2016-03-16    207\n",
       "2016-04-03    479\n",
       "2016-05-03    464\n",
       "2016-06-03    332\n",
       "2016-07-03    773\n",
       "2016-08-03    368\n",
       "2016-09-03    615\n",
       "2016-10-02    263\n",
       "2016-10-03    909\n",
       "2016-11-02    219\n",
       "2016-11-03    514\n",
       "2016-12-02     79\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(['publicationTime']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>publicationTime</th>\n",
       "      <th>bodyText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id:twitter.com:2209799083</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>RT @350Australia: Adani Group's Aust #coal min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>id:twitter.com:2209799083</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>RT @avivaimhof: Poor old #coal. Now even #Viet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>id:twitter.com:43060113</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>RT @market_forces: Funds have burned billions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>id:twitter.com:2209799083</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>RT @avivaimhof: #Vietnam PM Announces Retreat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>id:twitter.com:585560584</td>\n",
       "      <td>2016-10-02</td>\n",
       "      <td>RT @350Australia: Adani Group's Aust #coal min...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                     author publicationTime  \\\n",
       "0          0  id:twitter.com:2209799083      2016-10-02   \n",
       "1          1  id:twitter.com:2209799083      2016-10-02   \n",
       "2          2    id:twitter.com:43060113      2016-10-02   \n",
       "3          3  id:twitter.com:2209799083      2016-10-02   \n",
       "4          4   id:twitter.com:585560584      2016-10-02   \n",
       "\n",
       "                                            bodyText  \n",
       "0  RT @350Australia: Adani Group's Aust #coal min...  \n",
       "1  RT @avivaimhof: Poor old #coal. Now even #Viet...  \n",
       "2  RT @market_forces: Funds have burned billions ...  \n",
       "3  RT @avivaimhof: #Vietnam PM Announces Retreat ...  \n",
       "4  RT @350Australia: Adani Group's Aust #coal min...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove the URL's from bodyText\n",
    "data['Clean_Data'] = data['bodyText'].str.replace('http\\S+|www.\\S+', '', case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        RT @350Australia: Adani Group's Aust #coal min...\n",
      "1        RT @avivaimhof: Poor old #coal. Now even #Viet...\n",
      "2        RT @market_forces: Funds have burned billions ...\n",
      "3        RT @avivaimhof: #Vietnam PM Announces Retreat ...\n",
      "4        RT @350Australia: Adani Group's Aust #coal min...\n",
      "5        RT @QldGreens: How times change! Published on ...\n",
      "6        RT @LockTheGate: Ex #ICAC commish lays it on t...\n",
      "7        #Brisbane forum: Air #pollution, #coal &amp; p...\n",
      "8        #coal\\n4nature launched legal action against a...\n",
      "9        5 Climate Angels Arrested For ?Watching Over? ...\n",
      "10       RT @MineralsCouncil: Japan approves building o...\n",
      "11                    #auspol NO #coal #thorium #nuclear  \n",
      "12       RT @micksfoley: Govt's done a big #coal &amp; ...\n",
      "13       Possible Aus #LNG to India for #electricity vi...\n",
      "14       Australia's landholders are up in arms everywh...\n",
      "15       RT @AustralisTerry: Australia's landholders ar...\n",
      "16       RT @AustralisTerry: Australia's landholders ar...\n",
      "17       RT @AustralisTerry: Australia's landholders ar...\n",
      "18       Mining Weekly: #India?s #coal appetite dwindle...\n",
      "19       When PM &amp; incoming Deputy oppose #Shenhua ...\n",
      "20       RT @AustralisTerry: Australia's landholders ar...\n",
      "21       When PM &amp; incoming Deputy oppose #Shenhua ...\n",
      "22       RT @Phil_Laird1: When PM &amp; incoming Deputy...\n",
      "23       RT @PlattsCoal: Lender group offers $500 milli...\n",
      "24       Anon snr official @ #India's biggest power co ...\n",
      "25       RT @AustralisTerry: Australia's landholders ar...\n",
      "26       RT @AustralisTerry: Australia's landholders ar...\n",
      "27       RT @AustralisTerry: Australia's landholders ar...\n",
      "28       URGENT PETITION calling on the @NHMRC to study...\n",
      "29       RT @Phil_Laird1: When PM &amp; incoming Deputy...\n",
      "                               ...                        \n",
      "13364    @QRCouncil 's Sustainable Resource Investment ...\n",
      "13365    RT @jamiesonmurph: Whitehaven not implementing...\n",
      "13366    RT @maxphillips: VIDEO: Nannas to be gagged if...\n",
      "13367    RT @jamiesonmurph: Whitehaven not implementing...\n",
      "13368    RT @jamiesonmurph: Who gets the larger max jai...\n",
      "13369    RT @StopShenhua: ?The people?s resolve is such...\n",
      "13370    RT @nswnma: Protect the right to protest #nswp...\n",
      "13371    RT @nswnma: Protect the right to protest #nswp...\n",
      "13372    RT @jamiesonmurph: Santos contaminates an aqui...\n",
      "13373    RT @StopShenhua: ?The people?s resolve is such...\n",
      "13374    RT @jamiesonmurph: Santos contaminates an aqui...\n",
      "13375    RT @jamiesonmurph: Who gets the larger max jai...\n",
      "13376    RT @jamiesonmurph: Boggabri Coal illegally cle...\n",
      "13377    RT @jamiesonmurph: Santos contaminates an aqui...\n",
      "13378    RT @jamiesonmurph: Boggabri Coal illegally cle...\n",
      "13379    RT @JohnNorrisBrown: \"Documentary filmmaker to...\n",
      "13380    Why coal could be a better bet than baby food\\...\n",
      "13381    RT @StopShenhua: ?The people?s resolve is such...\n",
      "13382    RT @StopShenhua: ?The people?s resolve is such...\n",
      "13383    JPMorgan, Deutsche Bank set new #coal policies...\n",
      "13384    .@jpmorgan &amp; @DeutscheBank set new #coal p...\n",
      "13385    RT @StopShenhua: ?The people?s resolve is such...\n",
      "13386    RT @StopShenhua: \"The government is putting pu...\n",
      "13387    RT @StopShenhua: #shenhua #coal mine will clea...\n",
      "13388    RT @StopShenhua: This not only defines what @B...\n",
      "13389    RT @StopShenhua: Law Society says #nswpol prot...\n",
      "13390    RT @StopShenhua: \"The #liverpoolplains relies ...\n",
      "13391    RT @StopShenhua: Farmers &amp; #gomeroi people...\n",
      "13392    RT @PilligaPush: Protect the #pilliga forest a...\n",
      "13393    RT @StopShenhua: Law Society says #nswpol prot...\n",
      "Name: Clean_Data, Length: 13394, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['Clean_Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv('tweetoutput.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4a7a789da423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#from preprocessor import clean,tokenize,parse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'preprocessor'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import preprocessor as p\n",
    "\n",
    "#from preprocessor import clean,tokenize,parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweetoutput.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=re.compile(r'\\<http.+?\\>', re.DOTALL)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessor import clean, tokenize, parse\n",
    "df['Clean'] = df['Clean_Data'].apply(lambda x : p.clean(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(x):\n",
    "    APPOSTOPHES = {\"'s\" : \" is\", \"'re\" : \" are\", \":\" : \"\",\"&amp;\":\"\",\"?\":\"\",'\"':\"\"}\n",
    "    words = x.split()\n",
    "    reformed = [APPOSTOPHES[word] if word in APPOSTOPHES else word for word in words]\n",
    "    reformed = \" \".join(reformed)\n",
    "    return (reformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['Clean1'] = df['Clean_Data'].apply(lambda x : clean_data(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Clean1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine: 0.8616404368553293\n"
     ]
    }
   ],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     vec1 = text_to_vector(vec1)\n",
    "     vec2 = text_to_vector(vec2)\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "text1 = 'This is a foo bar sentence .'\n",
    "text2 = 'This sentence is similar to a foo bar sentence .'\n",
    "\n",
    "#vector1 = text_to_vector(text1)\n",
    "#vector2 = text_to_vector(text2)\n",
    "\n",
    "cosine = get_cosine(text1, text2)\n",
    "\n",
    "print('Cosine:', cosine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine = get_cosine(CosineData['Clean1'][0], CosineData['Clean1'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CosineData = pd.read_csv('Cosine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Cosine = ''\n",
    "CosineCoff = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CosineData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(CosineData)):\n",
    "    for j in range(len(CosineData)):\n",
    "        Cosine = get_cosine(CosineData['Clean1'][i], CosineData['Clean1'][j])\n",
    "        CosineCoff.append(Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.reshape(CosineCoff, (262, 262))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"1234.csv\", x, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import math\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jaccardCoff = []\n",
    "for i in range(len(CosineData)):\n",
    "    for j in range(len(CosineData)):\n",
    "        doc1 = tokenize(CosineData['Clean1'][i])\n",
    "        doc2 = tokenize(CosineData['Clean1'][j])\n",
    "        jacc = jaccard_similarity(doc1,doc2)\n",
    "        jaccardCoff.append(jacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.reshape(jaccardCoff, (262, 262))\n",
    "np.savetxt(\"jaccard.csv\", x, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_df= pd.read_csv('Data_1.csv')\n",
    "first_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence= first_df[['Clean1']]\n",
    "sentence['tokenized_sents'] = sentence.apply(lambda row1:nltk.word_tokenize(str(row1['Clean1'])), axis=1)\n",
    "print(sentence.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For finding the length of each text try to use apply and lambda function again:\n",
    "sentence['sents_length'] = sentence.apply(lambda row: len(row['tokenized_sents']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_df = first_df.reset_index(drop=True)\n",
    "sentence = sentence.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_df['tokenized_sents'] = sentence['tokenized_sents']\n",
    "first_df['sents_length'] = sentence['sents_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# convert to lower case\n",
    "lc = first_df[['tokenized_sents']]\n",
    "#lc = [w.lower() for w in lc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation\n",
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in first_df['tokenized_sents']:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaning text of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Stemming and Lemmatizing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_docs = []\n",
    "\n",
    "for doc in tokenized_docs_no_stopwords:\n",
    "    final_doc = []\n",
    "    for word in doc:\n",
    "        final_doc.append(porter.stem(word))\n",
    "    \n",
    "    preprocessed_docs.append(final_doc)\n",
    "\n",
    "print(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preprocessed_docs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocab\n",
    "#vectorizer.fit(preprocessed_docs)\n",
    "# summarize\n",
    "#print(vectorizer.vocabulary_)\n",
    "#print(vectorizer.idf_)\n",
    "#tfidf_matrix = vectorizer.fit_transform(preprocessed_docs)\n",
    "#print(tfidf_matrix)\n",
    "\n",
    "cvec = CountVectorizer(stop_words='english', min_df=1, max_df=.5, ngram_range=(1,2))\n",
    "print(cvec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(preprocessed_docs)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in preprocessed_docs]\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 3, id2word = dictionary, passes=50)\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 3, id2word = dictionary, passes=50)\n",
    "print(ldamodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = tfidf_matrix.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names, index=characters)\n",
    "s = pd.Series(df.loc['Adam'])\n",
    "s[s > 0].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import string\n",
    "import collections\n",
    "def cluster_texts(texts, clusters=3):\n",
    "    \"\"\" Transform texts to Tf-Idf coordinates and cluster texts using K-Means \"\"\"\n",
    "    vectorizer = TfidfVectorizer(tokenizer=tokenized_docs_no_punctuation,\n",
    "                                 stop_words=stopwords.words('english'),\n",
    "                                 max_df=0.5,\n",
    "                                 min_df=0.1,\n",
    "                                 lowercase=True)\n",
    " \n",
    "    tfidf_model = vectorizer.fit_transform(texts)\n",
    "    km_model = KMeans(n_clusters=clusters)\n",
    "    km_model.fit(tfidf_model)\n",
    " \n",
    "    clustering = collections.defaultdict(list)\n",
    " \n",
    "    for idx, label in enumerate(km_model.labels_):\n",
    "        clustering[label].append(idx)\n",
    " \n",
    "    return clustering\n",
    "    print(clustering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

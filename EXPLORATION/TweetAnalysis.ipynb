{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, re, math #used throughout\n",
    "from collections import Counter #used in jaccard\n",
    "from __future__ import division #used in TF_IDF\n",
    "import string #_used in TF_IDF\n",
    "import os   #used to check for existence of files\n",
    "import statistics #used to calculate means\n",
    "#from itertools import groupby #used in early attempt   at grouping keywords\n",
    "import operator  #use in to count key words   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tweets from .csv file\n",
    "\n",
    "## Arrange Tweets by Date and Strip URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfilename = 'realFinalNHB.csv'\n",
    "\n",
    "tweetList = []\n",
    "with open(inputfilename, 'r', encoding='latin-1') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        tweetList.append(row)\n",
    "\n",
    "\n",
    "def remURL(tweetTxt):\n",
    "    url=\"https*\\S*\" #many of the tweets contain no colon or slashes after http[s]\n",
    "    return re.sub(url, '', tweetTxt)\n",
    "\n",
    "allTweetsByDate={}\n",
    "allTweetsByDate[tweetList[0]['publicationTime'][:8]] = []\n",
    "for dic in tweetList:\n",
    "    dateT=dic['publicationTime'][:8] \n",
    "    if dateT in allTweetsByDate.keys():\n",
    "        allTweetsByDate[dateT].append(dic['bodyText'])\n",
    "    else:\n",
    "        allTweetsByDate[dateT]=[dic['bodyText']]\n",
    "for key in allTweetsByDate.keys():\n",
    "    for  n in range(len(allTweetsByDate[key])):\n",
    "        allTweetsByDate[key][n]= remURL(allTweetsByDate[key][n])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise comparison function. Used for both cosine and Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairwiseCompare (compFunction, stringList):\n",
    "    resultMatrix = []\n",
    "    for s in stringList:\n",
    "        resultList = []\n",
    "        for S in stringList:\n",
    "            resultList.append(compFunction(s,S))\n",
    "        resultMatrix.append(resultList)\n",
    "    return resultMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Function to calculate cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(string1, string2):\n",
    "     vec1 = text_to_vector(string1)\n",
    "     vec2 = text_to_vector(string2)\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test pairwise comparison, cosine similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 0.7071067811865475, 0.0],\n",
       " [0.7071067811865475, 0.9999999999999998, 0.0],\n",
       " [0.0, 0.0, 1.0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwiseCompare(get_cosine, [\"hello\", \"hello world\", \"lololol \"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "def jaccard_similarity(string1, string2):\n",
    "    query = tokenize(string1)\n",
    "    document = tokenize(string2)\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)\n",
    "# pairwiseCompare(jaccard_similarity, [\"hello\", \"hello world\", \"lololol \"]) #test: outputs [[1.0, 0.5, 0.0], [0.5, 1.0, 0.0], [0.0, 0.0, 1.0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF_IDF\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n",
    "\n",
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    try:\n",
    "        return 1 + math.log(tokenized_document.count(term))\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    "\n",
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values\n",
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents\n",
    "#tfidf([\"hello\", \"hello world\", \"lololol \"]) #test. outputs [[2.09861228866811, 1.4054651081081644, 2.09861228866811, 2.09861228866811],\n",
    " #[2.09861228866811, 1.4054651081081644, 2.09861228866811, 2.09861228866811],\n",
    " #[2.09861228866811, 1.4054651081081644, 2.09861228866811, 2.09861228866811]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to calculate average for cosine, Jaccard, and TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matrixmean(matrix):\n",
    "    mylist=[]\n",
    "    for row in matrix:\n",
    "        mylist.append(statistics.mean(row))\n",
    "    return statistics.mean(mylist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pairwise cos similarity to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrixmean(matrix):\n",
    "    mylist=[]\n",
    "    for row in matrix:\n",
    "        mylist.append(statistics.mean(row))\n",
    "    return statistics.mean(mylist)\n",
    "        \n",
    "def writePairwiseCos(dateKey):\n",
    "    filekey=re.sub('/','', dateKey)\n",
    "    filename='cosine/pairwiseCos' + filekey + '.csv'\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, dialect = 'excel')\n",
    "            todaysTweets = allTweetsByDate [dateKey]\n",
    "            todayCos = pairwiseCompare(get_cosine, todaysTweets)\n",
    "            for row in todayCos:\n",
    "                writer.writerow(row)\n",
    "        csvfile.close()\n",
    "        return  matrixmean (todayCos)\n",
    "    else:\n",
    "        return -1 #indicates error. Presence of -1 in cosAverages list means writePairwiseCos has been called too many times\n",
    "!mkdir cosine\n",
    "\n",
    "cosAverages = []\n",
    "for key in allTweetsByDate.keys():\n",
    "    cosAverages.append(writePairwiseCos(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pairwise Jaccard similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePairwiseJaccard(dateKey):\n",
    "    filekey=re.sub('/','', dateKey)\n",
    "    filename='jaccard/pairwiseJaccard' + filekey + '.csv'\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, dialect = 'excel')\n",
    "            todaysTweets = allTweetsByDate [dateKey]\n",
    "            todayJaccard = pairwiseCompare(jaccard_similarity, todaysTweets)\n",
    "            for row in todayJaccard:\n",
    "                writer.writerow(row)\n",
    "        csvfile.close()\n",
    "        return  matrixmean (todayJaccard)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "!mkdir jaccard\n",
    "jaccardAverages=[]\n",
    "for key in allTweetsByDate.keys():\n",
    "    jaccardAverages.append(writePairwiseJaccard(key))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write pairwise TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_tfidf(dateKey):\n",
    "    filekey=re.sub('/','', dateKey)\n",
    "    filename='tfidf/pairwiseTFIDF' + filekey + '.csv'\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, dialect = 'excel')\n",
    "            todaysTweets = allTweetsByDate [dateKey]\n",
    "            todayTFIDF = tfidf(todaysTweets)\n",
    "            for row in todayTFIDF:\n",
    "                writer.writerow(row)\n",
    "            csvfile.close()\n",
    "        return matrixmean(todayTFIDF)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "tfidfAverages = []\n",
    "for key in allTweetsByDate.keys():\n",
    "    tfidfAverages.append(write_tfidf(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write daily averages to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!touch dailyaverages.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "averagesList =[]\n",
    "datecount=0\n",
    "for dateKey in allTweetsByDate.keys(): \n",
    "    dailyAverages = [dateKey, cosAverages[datecount], jaccardAverages[datecount], tfidfAverages[datecount]]\n",
    "    averagesList.append(dailyAverages)\n",
    "    datecount += 1\n",
    "\n",
    "with open(\"dailyaverages.csv\", 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)  \n",
    "    for row in averagesList:\n",
    "        writer.writerow(row)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(string):\n",
    "    return re.findall(r\"[\\w']+\", string.upper())\n",
    "\n",
    "stoplistfilename = \"stopList.txt\"\n",
    "\n",
    "with open(stoplistfilename, 'r') as file:\n",
    "    rawStop = file.read()\n",
    "stopwords = tokens(rawStop)\n",
    "\n",
    "print(stopwords) #test: returns ['THE', 'A', 'IN'] for test stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripStopWords(tokenList):\n",
    "    result=[]\n",
    "    for word in tokenList:\n",
    "        if (not (word in stopwords)):\n",
    "            result.append(word)\n",
    "    return result\n",
    "#print(stripStopWords(tokens(\"The unicycle is in a shop that is the best\"))) #test: returns ['UNICYCLE', 'IS', 'SHOP', 'THAT', 'IS', 'BEST'] for test stoplist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[len(list(group)) for key, group in groupby(a)]\n",
    "def topTenWords(listOfTweets):\n",
    "    \"\"\"Relies on presence of global variable stopwords\"\"\"\n",
    "    tok =  []\n",
    "    for tweet in listOfTweets:\n",
    "        stripped = stripStopWords(tokens(tweet))\n",
    "        tok += stripped\n",
    "    tokDict = Counter(tok)\n",
    "    tokCounts = list(reversed(sorted(tokDict.items(), key=operator.itemgetter(1)))) #list of tuples (token, # of occurrences)\n",
    "    topTen = tokCounts[:10]\n",
    "    return topTen\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(topTenWords([\"hello hi\", \"hello goodbye plastic hello\", \"hello my cat's breath smells like cat food\", \"hello plastic I ate too much plastic candy\", \"Hello Joe!\", \"I pandas\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir topten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_topten(dateKey):\n",
    "    filekey=re.sub('/','', dateKey)\n",
    "    filename='topten/toptenfor' + filekey + '.csv'\n",
    "    if not os.path.isfile(filename):\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, dialect = 'excel')\n",
    "            todaysTweets = allTweetsByDate [dateKey]\n",
    "            todayTopTen = topTenWords(todaysTweets)\n",
    "            for row in todayTopTen:\n",
    "                writer.writerow(row)\n",
    "            csvfile.close()\n",
    "for key in allTweetsByDate.keys():\n",
    "    write_topten(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby([\"hello\", \"goodbye\", \"hi\", \"hello\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

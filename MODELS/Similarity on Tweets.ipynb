{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine: 0.372677996249965\n"
     ]
    }
   ],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "#text1 = 'This is a foo bar sentence .'\n",
    "#text2 = 'This sentence is similar to a foo bar sentence .'\n",
    "\n",
    "\n",
    "text1 = \"\"\"\"Menteri Luhut Ungkap Dasar Pertimbangan Beri Izin Ekspor Freeport https://t.co/3oEKZZblDL, JAKARTA? https://t.co/JcK9tfe5fm https://t.co/xzINyEOOjc\n",
    "\"\"\"\n",
    "\n",
    "text2= \"\"\" \n",
    "Luhut Yakin Izin Sementara untuk Freeport tidak Melanggar Hukum https://t.co/3oEKZZblDL, JAKARTA -? https://t.co/cGoYyMcnPh https://t.co/lNr8bFoaye\n",
    "Oleh : Ninik Yuniati\n",
    "KBR, Jakarta- Pemerintah Provinsi Papua meminta PT Freeport Indonesia segera membayar... https://t.co/OXEnfMBwrc\"\n",
    "\"\"\"\n",
    "text3  = \"\"\"\n",
    "Oleh : Ninik Yuniati\n",
    "KBR, Jakarta- Pemerintah Provinsi Papua meminta PT Freeport Indonesia segera membayar... https://t.co/K5HqSdvExI\"\n",
    "\"\"\"\n",
    "\n",
    "vector1 = text_to_vector(text1)\n",
    "vector2 = text_to_vector(text2)\n",
    "vector3 = text_to_vector(text3)\n",
    "\n",
    "cosine = get_cosine(vector1, vector3)\n",
    "\n",
    "print('Cosine:', cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating World Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menteri 1\n",
      "luhut 2\n",
      "ungkap 1\n",
      "dasar 1\n",
      "pertimbangan 1\n",
      "beri 1\n",
      "izin 2\n",
      "ekspor 1\n",
      "freeport 4\n",
      "https 8\n",
      "jakarta 4\n",
      "xzinyeoojc 1\n",
      "yakin 1\n",
      "sementara 1\n",
      "untuk 1\n",
      "tidak 1\n",
      "melanggar 1\n",
      "hukum 1\n",
      "cgoyymcnph 1\n",
      "oleh 2\n",
      "ninik 2\n",
      "yuniati 2\n",
      "kbr 2\n",
      "pemerintah 2\n",
      "provinsi 2\n",
      "papua 2\n",
      "meminta 2\n",
      "indonesia 2\n",
      "segera 2\n",
      "membayar 2\n",
      "oxenfmbwrc 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https</th>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freeport</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jakarta</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yuniati</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provinsi</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pemerintah</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>papua</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oleh</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ninik</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meminta</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>membayar</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luhut</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kbr</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>izin</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indonesia</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>segera</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ungkap</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tidak</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>untuk</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sementara</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xzinyeoojc</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yakin</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beri</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pertimbangan</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oxenfmbwrc</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cgoyymcnph</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>melanggar</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hukum</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ekspor</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dasar</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>menteri</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              A B\n",
       "https         8  \n",
       "freeport      4  \n",
       "jakarta       4  \n",
       "yuniati       2  \n",
       "provinsi      2  \n",
       "pemerintah    2  \n",
       "papua         2  \n",
       "oleh          2  \n",
       "ninik         2  \n",
       "meminta       2  \n",
       "membayar      2  \n",
       "luhut         2  \n",
       "kbr           2  \n",
       "izin          2  \n",
       "indonesia     2  \n",
       "segera        2  \n",
       "ungkap        1  \n",
       "tidak         1  \n",
       "untuk         1  \n",
       "sementara     1  \n",
       "xzinyeoojc    1  \n",
       "yakin         1  \n",
       "beri          1  \n",
       "pertimbangan  1  \n",
       "oxenfmbwrc    1  \n",
       "cgoyymcnph    1  \n",
       "melanggar     1  \n",
       "hukum         1  \n",
       "ekspor        1  \n",
       "dasar         1  \n",
       "menteri       1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "frequency = {}\n",
    "document_text = open('test.txt', 'r')\n",
    "#document_text = \"The boy is going\"\n",
    "text_string = document_text.read().lower()\n",
    "match_pattern = re.findall(r'\\b[a-z]{3,15}\\b', text_string)\n",
    " \n",
    "for word in match_pattern:\n",
    "# h = [(words), (frequency[words])]\n",
    "    count = frequency.get(word,0)\n",
    "    frequency[word] = count + 1\n",
    "    \n",
    "frequency_list = frequency.keys()\n",
    " \n",
    "for words in frequency_list:\n",
    "    print(words, frequency[words])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data={'A': frequency, 'B': \"\" })\n",
    "df.sort_values(by=['A'], ascending=False)\n",
    "#df\n",
    "#df.sort(['A', 'B'], ascending= False)\n",
    "#frequency[words]\n",
    "#frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n",
      "sample 1\n",
      "sentence 1\n",
      "showing 1\n",
      "stop 1\n",
      "words 1\n",
      "filtration 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "frequency = {}\n",
    "#document_text = open('test.txt', 'r')\n",
    "#document_text = \"The boy is going\"\n",
    "#text_string = document_text.read().lower()\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)\n",
    "#filtered_sentence.tokenize\n",
    "\n",
    "str1 = ' '.join(filtered_sentence)\n",
    "#str1 = ''.join(str(e) for e in filtered_sentence)\n",
    "\n",
    "match_pattern = re.findall(r'\\b[a-z]{3,15}\\b', str1)\n",
    "\n",
    "for word in match_pattern:\n",
    "    count = frequency.get(word,0)\n",
    "    frequency[word] = count + 1\n",
    "     \n",
    "frequency_list = frequency.keys()\n",
    " \n",
    "for words in frequency_list:\n",
    "    print(words, frequency[words])\n",
    "    \n",
    "#type(filtered_sentence)\n",
    "type(example_sent)#\n",
    "#type(str1)\n",
    "#str1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Code FOR Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menteri 1\n",
      "luhut 2\n",
      "ungkap 1\n",
      "dasar 1\n",
      "pertimbangan 1\n",
      "beri 1\n",
      "izin 2\n",
      "ekspor 1\n",
      "freeport 4\n",
      "https 8\n",
      "jakarta 4\n",
      "xzinyeoojc 1\n",
      "yakin 1\n",
      "sementara 1\n",
      "untuk 1\n",
      "tidak 1\n",
      "melanggar 1\n",
      "hukum 1\n",
      "cgoyymcnph 1\n",
      "oleh 2\n",
      "ninik 2\n",
      "yuniati 2\n",
      "kbr 2\n",
      "pemerintah 2\n",
      "provinsi 2\n",
      "papua 2\n",
      "meminta 2\n",
      "indonesia 2\n",
      "segera 2\n",
      "membayar 2\n",
      "oxenfmbwrc 1\n",
      "**********************************************\n",
      "menteri luhut ungkap dasar pertimbangan beri izin ekspor freeport https co 3oekzzbldl jakarta https co jck9tfe5fm https co xzinyeoojc luhut yakin izin sementara untuk freeport tidak melanggar hukum https co 3oekzzbldl jakarta https co cgoyymcnph https co lnr8bfoaye oleh ninik yuniati kbr jakarta pemerintah provinsi papua meminta pt freeport indonesia segera membayar https co k5hqsdvexi oleh ninik yuniati kbr jakarta pemerintah provinsi papua meminta pt freeport indonesia segera membayar https co oxenfmbwrc\n",
      "update 2\n",
      "indonesia 4\n",
      "says 4\n",
      "freeport 5\n",
      "miners 4\n",
      "halt 4\n",
      "exports 4\n",
      "jakarta 2\n",
      "seeks 1\n",
      "ease 1\n",
      "ban 1\n",
      "still 1\n",
      "finalising 1\n",
      "copper 1\n",
      "yahoo 1\n",
      "finance 1\n",
      "jan 1\n",
      "scale 1\n",
      "destruction 1\n",
      "mine 1\n",
      "caused 1\n",
      "visible 1\n",
      "space 1\n",
      "environmental 1\n",
      "genocide 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ban</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caused</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>copper</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>destruction</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ease</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>environmental</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exports</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finalising</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freeport</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genocide</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>halt</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indonesia</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jakarta</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jan</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mine</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>miners</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>says</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scale</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seeks</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>space</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>still</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>update</th>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visible</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               A B\n",
       "ban            1  \n",
       "caused         1  \n",
       "copper         1  \n",
       "destruction    1  \n",
       "ease           1  \n",
       "environmental  1  \n",
       "exports        4  \n",
       "finalising     1  \n",
       "finance        1  \n",
       "freeport       5  \n",
       "genocide       1  \n",
       "halt           4  \n",
       "indonesia      4  \n",
       "jakarta        2  \n",
       "jan            1  \n",
       "mine           1  \n",
       "miners         4  \n",
       "says           4  \n",
       "scale          1  \n",
       "seeks          1  \n",
       "space          1  \n",
       "still          1  \n",
       "update         2  \n",
       "visible        1  \n",
       "yahoo          1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "frequency = {}\n",
    "import string\n",
    "frequency = {}\n",
    "frequency1= {}\n",
    "document_text = open('test.txt', 'r')\n",
    "document_text1 = open('xtest.txt', 'r')\n",
    "#document_text = \"The boy is going\"\n",
    "text_string = document_text.read().lower()\n",
    "text_string1 = document_text1.read().lower()\n",
    "\n",
    "#clean all Url from texts.\n",
    "#result = re.sub(r\"http\\S+\", \" \",  text_string)\n",
    "#result = re.sub(r\"[^A-Za-z0-9]+\", \" \",  text_string)\n",
    "result = re.sub(r\"\\W+\", \" \",  text_string)\n",
    "result1 = re.sub(r\"\\W+\", \" \",  text_string1)\n",
    "\n",
    "\n",
    "#type(result)\n",
    "# Stop Word Removal \n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(result)\n",
    "word_tokens1 = word_tokenize(result1)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "filtered_sentence1 = [x for x in word_tokens1 if not x in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "filtered_sentence1 = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "\n",
    "for x in word_tokens1:\n",
    "    if x not in stop_words:\n",
    "        filtered_sentence1.append(x)\n",
    "#print(word_tokens)\n",
    "#print(filtered_sentence)\n",
    "\n",
    "#Now Perform Count on it words\n",
    "str1 = ' '.join(filtered_sentence)\n",
    "str2 = ' '.join(filtered_sentence1)\n",
    "#str1 = ''.join(str(e) for e in filtered_sentence)\n",
    "\n",
    "match_pattern = re.findall(r'\\b[a-z]{3,15}\\b', str1)\n",
    "match_pattern1 = re.findall(r'\\b[a-z]{3,15}\\b', str2)\n",
    "\n",
    "for word in match_pattern:\n",
    "    count = frequency.get(word,0)\n",
    "    frequency[word] = count + 1\n",
    "    \n",
    "for wordx in match_pattern1:\n",
    "    count1 = frequency1.get(wordx,0)\n",
    "    frequency1[wordx] = count1 + 1\n",
    "         \n",
    "frequency_list = frequency.keys()\n",
    "frequency_list1 = frequency1.keys()\n",
    " \n",
    "for words in frequency_list:\n",
    "    print(words, frequency[words])\n",
    "print(\"**********************************************\")  \n",
    "print(str1)\n",
    "for wordsx in frequency_list1:\n",
    "    print(wordsx, frequency1[wordsx])\n",
    "\n",
    "df = pd.DataFrame(data={'A': frequency1, 'B': \"\" })\n",
    "#df.sort_values(by=['A'], ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'update 2 indonesia says freeport other miners halt exports jakarta seeks to ease ban but still finalising update 1 indonesia says freeport other copper miners halt exports indonesia says freeport other miners halt exports yahoo finance jakarta jan 12 indonesia says freeport other miners halt exports he scale and destruction that the freeport mine has caused visible from space environmental genocide'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calculate the Cosine Similarities between 2 Trees Text generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine: 0.8616404368553293\n"
     ]
    }
   ],
   "source": [
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "     intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "     numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "     sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "     sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "     denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "     if not denominator:\n",
    "        return 0.0\n",
    "     else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "     words = WORD.findall(text)\n",
    "     return Counter(words)\n",
    "\n",
    "text1 = 'This is a foo bar sentence .'\n",
    "text2 = 'This sentence is similar to a foo bar sentence .'\n",
    "\n",
    "\n",
    "\n",
    "vector1 = text_to_vector(text1)\n",
    "vector2 = text_to_vector(text2)\n",
    "\n",
    "cosine = get_cosine(vector1, vector2)\n",
    "\n",
    "print('Cosine:', cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacccard Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import string\n",
    "import math\n",
    "\n",
    "tokenize = lambda doc: doc.lower().split(\" \")\n",
    "document_0 = \"China has a strong economy that is growing at a rapid pace. However politically it differs greatly from the US Economy.\"\n",
    "document_1 = \"At last, China seems serious about confronting an endemic problem: domestic violence and corruption.\"\n",
    "document_2 = \"Japan's prime minister, Shinzo Abe, is working towards healing the economic turmoil in his own country for his view on the future of his people.\"\n",
    "document_3 = \"Vladimir Putin is working hard to fix the economy in Russia as the Ruble has tumbled.\"\n",
    "document_4 = \"What's the future of Abenomics? We asked Shinzo Abe for his views\"\n",
    "document_5 = \"Obama has eased sanctions on Cuba while accelerating those against the Russian Economy, even as the Ruble's value falls almost daily.\"\n",
    "document_6 = \"Vladimir Putin was found to be riding a horse, again, without a shirt on while hunting deer. Vladimir Putin always seems so serious about things - even riding horses.\"\n",
    "\n",
    "text1 = 'This is a foo bar sentence .'\n",
    "text2 = 'This sentence is similar to a foo bar sentence .'\n",
    "#all_documents = [document_0, document_1, document_2, document_3, document_4, document_5, document_6]\n",
    "all_documents = [text1,text2]\n",
    "tokenized_documents = [tokenize(d) for d in all_documents] # tokenized docs\n",
    "all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return len(intersection)/len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7777777777777778"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity(tokenized_documents[0],tokenized_documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1',\n",
       " 'copper',\n",
       " 'ease',\n",
       " 'exports',\n",
       " 'freeport',\n",
       " 'halt',\n",
       " 'indonesia',\n",
       " 'jakarta',\n",
       " 'miners',\n",
       " 'says',\n",
       " 'seeks',\n",
       " 'update'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate the lists of documents\n",
    "set(tokenized_documents[0]).intersection(set(tokenized_documents[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF_IDF\n",
    "def term_frequency(term, tokenized_document):\n",
    "    return tokenized_document.count(term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sublinear_term_frequency(term, tokenized_document):\n",
    "    return 1 + math.log(tokenized_document.count(term))\n",
    "\n",
    "def augmented_term_frequency(term, tokenized_document):\n",
    "    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inverse_document_frequencies(tokenized_documents):\n",
    "    idf_values = {}\n",
    "    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "    for tkn in all_tokens_set:\n",
    "        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "    return idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6931471805599454\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "idf_values = inverse_document_frequencies(tokenized_documents)\n",
    "print(idf_values['australia'])\n",
    "\n",
    "print(idf_values['freeport'])\n",
    "# 1.33647223662"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(documents):\n",
    "    tokenized_documents = [tokenize(d) for d in documents]\n",
    "    idf = inverse_document_frequencies(tokenized_documents)\n",
    "    tfidf_documents = []\n",
    "    for document in tokenized_documents:\n",
    "        doc_tfidf = []\n",
    "        for term in idf.keys():\n",
    "            tf = sublinear_term_frequency(term, document)\n",
    "            doc_tfidf.append(tf * idf[term])\n",
    "        tfidf_documents.append(doc_tfidf)\n",
    "    return tfidf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 2.9459101490553135, 2.9459101490553135, 0.0, 2.9459101490553135, 0.0, 0.0, 2.9459101490553135, 1.5596157879354227, 0.0, 0.0, 0.0, 3.8142592685777856, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8472978603872037, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 1.336472236621213, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0] australia respect indonesia period auspol respect territorial integrity freeport mcmoran westpapua australia respect indonesia period auspol respect territorial integrity freeport mcmoran westpapua https co uop2frzj0v update 1 indonesia says freeport copper miners halt exports jakarta seeks ease usa market news https co fpnc2cmjnp\n"
     ]
    }
   ],
   "source": [
    "tfidf_representation = tfidf(all_documents)\n",
    "print(tfidf_representation[0], str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 2.9459101490553135, 2.9459101490553135, 0.0, 2.9459101490553135, 0.0, 0.0, 2.9459101490553135, 1.5596157879354227, 0.0, 0.0, 0.0, 3.8142592685777856, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.8472978603872037, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 1.336472236621213, 0.0, 2.9459101490553135, 0.0, 0.0, 0.0, 0.0, 2.252762968495368, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.3112012154045971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18380045100490253, 0.18380045100490253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2403535665263221, 0.0, 0.0, 0.0, 0.18380045100490253, 0.0, 0.2403535665263221, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2403535665263221, 0.0, 0.2403535665263221, 0.2403535665263221, 0.0, 0.15071899912592052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2403535665263221, 0.0, 0.0, 0.127247335483483, 0.2403535665263221, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2403535665263221, 0.0, 0.2403535665263221, 0.0, 0.0, 0.0, 0.2403535665263221, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2403535665263221, 0.2403535665263221, 0.10904129874372748, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2403535665263221, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "australia respect indonesia period auspol respect territorial integrity freeport mcmoran westpapua australia respect indonesia period auspol respect territorial integrity freeport mcmoran westpapua https co uop2frzj0v update 1 indonesia says freeport copper miners halt exports jakarta seeks ease usa market news https co fpnc2cmjnp\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_representation[0])\n",
    "print(sklearn_representation.toarray()[0].tolist())\n",
    "print(str1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2):\n",
    "    dot_product = sum(p*q for p,q in zip(vector1, vector2))\n",
    "    magnitude = math.sqrt(sum([val**2 for val in vector1])) * math.sqrt(sum([val**2 for val in vector2]))\n",
    "    if not magnitude:\n",
    "        return 0\n",
    "    return dot_product/magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1.0000000000000002, 4, 4), (1.0000000000000002, 6, 6))\n",
      "((1.0000000000000002, 1, 1), (1.0000000000000002, 2, 2))\n",
      "((1.0, 6, 6), (1.0000000000000002, 0, 0))\n",
      "((1.0, 5, 5), (1.0, 5, 5))\n",
      "((1.0, 3, 3), (1.0, 4, 4))\n",
      "((1.0, 0, 0), (1.0, 3, 3))\n",
      "((0.9999999999999998, 2, 2), (1.0, 1, 1))\n",
      "((0.29310925698840584, 4, 2), (0.29310925698840595, 4, 2))\n",
      "((0.29310925698840584, 2, 4), (0.29310925698840595, 2, 4))\n",
      "((0.1650630690646461, 6, 3), (0.16506306906464616, 6, 3))\n",
      "((0.1650630690646461, 3, 6), (0.16506306906464616, 3, 6))\n",
      "((0.14060334967136978, 3, 2), (0.14060334967136984, 3, 2))\n",
      "((0.14060334967136978, 2, 3), (0.14060334967136984, 2, 3))\n",
      "((0.11766551247749864, 3, 0), (0.11766551247749867, 3, 0))\n",
      "((0.11766551247749864, 0, 3), (0.11766551247749867, 0, 3))\n",
      "((0.11478807222952392, 5, 3), (0.11478807222952396, 5, 3))\n",
      "((0.11478807222952392, 3, 5), (0.11478807222952396, 3, 5))\n",
      "((0.11212208176085793, 6, 1), (0.11212208176085793, 6, 1))\n",
      "((0.11212208176085793, 1, 6), (0.11212208176085793, 1, 6))\n",
      "((0.08140732228934985, 1, 0), (0.08140732228934984, 1, 0))\n",
      "((0.08140732228934985, 0, 1), (0.08140732228934984, 0, 1))\n",
      "((0.07769257358999192, 6, 0), (0.077692573589991931, 6, 0))\n",
      "((0.07769257358999192, 0, 6), (0.077692573589991931, 0, 6))\n",
      "((0.0638569521562436, 5, 2), (0.063856952156243624, 5, 2))\n",
      "((0.0638569521562436, 2, 5), (0.063856952156243624, 2, 5))\n",
      "((0.062016241985445496, 6, 5), (0.062016241985445517, 6, 5))\n",
      "((0.062016241985445496, 5, 6), (0.062016241985445517, 5, 6))\n",
      "((0.042832542061084215, 5, 0), (0.042832542061084229, 5, 0))\n",
      "((0.042832542061084215, 0, 5), (0.042832542061084229, 0, 5))\n",
      "((0.03513943454371288, 4, 3), (0.035139434543712877, 4, 3))\n",
      "((0.03513943454371288, 3, 4), (0.035139434543712877, 3, 4))\n",
      "((0.03418867924527534, 2, 0), (0.034188679245275355, 2, 0))\n",
      "((0.03418867924527534, 0, 2), (0.034188679245275355, 0, 2))\n",
      "((0.027710443726288548, 5, 4), (0.027710443726288551, 5, 4))\n",
      "((0.027710443726288548, 4, 5), (0.027710443726288551, 4, 5))\n",
      "((0.023693226722828944, 6, 2), (0.023693226722828954, 6, 2))\n",
      "((0.023693226722828944, 2, 6), (0.023693226722828954, 2, 6))\n",
      "((0.016372043415710746, 4, 0), (0.016372043415710743, 4, 0))\n",
      "((0.016372043415710746, 0, 4), (0.016372043415710743, 0, 4))\n",
      "((0.0, 6, 4), (0.0, 6, 4))\n",
      "((0.0, 5, 1), (0.0, 5, 1))\n",
      "((0.0, 4, 6), (0.0, 4, 6))\n",
      "((0.0, 4, 1), (0.0, 4, 1))\n",
      "((0.0, 3, 1), (0.0, 3, 1))\n",
      "((0.0, 2, 1), (0.0, 2, 1))\n",
      "((0.0, 1, 5), (0.0, 1, 5))\n",
      "((0.0, 1, 4), (0.0, 1, 4))\n",
      "((0.0, 1, 3), (0.0, 1, 3))\n",
      "((0.0, 1, 2), (0.0, 1, 2))\n"
     ]
    }
   ],
   "source": [
    "fidf_representation = tfidf(all_documents)\n",
    "our_tfidf_comparisons = []\n",
    "for count_0, doc_0 in enumerate(tfidf_representation):\n",
    "    for count_1, doc_1 in enumerate(tfidf_representation):\n",
    "        our_tfidf_comparisons.append((cosine_similarity(doc_0, doc_1), count_0, count_1))\n",
    "\n",
    "skl_tfidf_comparisons = []\n",
    "for count_0, doc_0 in enumerate(sklearn_representation.toarray()):\n",
    "    for count_1, doc_1 in enumerate(sklearn_representation.toarray()):\n",
    "        skl_tfidf_comparisons.append((cosine_similarity(doc_0, doc_1), count_0, count_1))\n",
    "\n",
    "for x in zip(sorted(our_tfidf_comparisons, reverse = True), sorted(skl_tfidf_comparisons, reverse = True)):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"), (\"dog\", \"NN\"), (\"barked\",\"VBD\"), (\"at\", \"IN\"), (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "pattern = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "NPChunker = nltk.RegexpParser(pattern) \n",
    "result = NPChunker.parse(sentence)\n",
    "#result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Australia/NNP)\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  respect/VB\n",
      "  (GPE Indonesia/NNP)\n",
      "  ,/,\n",
      "  period/NN\n",
      "  ./.\n",
      "  #/#\n",
      "  (ORGANIZATION AusPol/NNP)\n",
      "  respect/VBP\n",
      "  the/DT\n",
      "  territorial/JJ\n",
      "  integrity/NN\n",
      "  of/IN\n",
      "  (ORGANIZATION Freeport/NNP)\n",
      "  McMoran/NNP\n",
      "  #/#\n",
      "  (ORGANIZATION WestPapua/NNP Australia/NNP)\n",
      "  do/VBP\n",
      "  n't/RB\n",
      "  respect/VB\n",
      "  (GPE Indonesia/NNP)\n",
      "  ,/,\n",
      "  period/NN\n",
      "  ./.\n",
      "  #/#\n",
      "  (ORGANIZATION AusPol/NNP)\n",
      "  respect/VBP\n",
      "  the/DT\n",
      "  territorial/JJ\n",
      "  integrity/NN\n",
      "  of/IN\n",
      "  (ORGANIZATION Freeport/NNP)\n",
      "  McMoran/NNP\n",
      "  #/#\n",
      "  (ORGANIZATION WestPapua/NNP)\n",
      "  https/NN\n",
      "  :/:\n",
      "  //t.co/uOp2fRzj0V/JJ\n",
      "  (ORGANIZATION UPDATE/NNP)\n",
      "  1-Indonesia/CD\n",
      "  says/VBZ\n",
      "  (PERSON Freeport/NNP)\n",
      "  ,/,\n",
      "  other/JJ\n",
      "  copper/NN\n",
      "  miners/NNS\n",
      "  halt/VBD\n",
      "  exports/NNS\n",
      "  :/:\n",
      "  */NN\n",
      "  (GPE Jakarta/NNP)\n",
      "  seeks/VBZ\n",
      "  to/TO\n",
      "  ease../VB\n",
      "  #/#\n",
      "  usa/JJ\n",
      "  #/#\n",
      "  market/NN\n",
      "  #/#\n",
      "  news/NN\n",
      "  https/NN\n",
      "  :/:\n",
      "  //t.co/fPnC2cmJNP/NN\n",
      "  (PERSON Jeff/NNP AMos/NNP)\n",
      "  JAmes/NNP)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nltk.tree.Tree"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "with open('test.txt', 'r') as f:\n",
    "    sample = f.read()\n",
    "print(ne_chunk(pos_tag(word_tokenize(sample))))\n",
    "\n",
    "type(ne_chunk(pos_tag(word_tokenize(str1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "with open('test.txt', 'r') as f:\n",
    "    sample = f.read()\n",
    "#sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Australia', 'Indonesia']\n",
      "['AusPol', 'Freeport', 'WestPapua Australia', 'Indonesia']\n",
      "['AusPol', 'Freeport', 'WestPapua', 'Freeport', 'Jakarta', 'Jeff']\n",
      "['Australia', 'Indonesia', 'AusPol', 'Freeport', 'WestPapua Australia', 'Indonesia', 'AusPol', 'Freeport', 'WestPapua', 'Freeport', 'Jakarta', 'Jeff']\n",
      "{'Jakarta', 'Australia', 'Indonesia', 'Jeff', 'WestPapua', 'AusPol', 'Freeport', 'WestPapua Australia'}\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "with open('test.txt', 'r') as f:\n",
    "    sample = f.read()\n",
    "\n",
    "sentences = nltk.sent_tokenize(sample)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "chunked_sentences = nltk.ne_chunk_sents(tagged_sentences, binary=True)\n",
    "\n",
    "def extract_entity_names(t):\n",
    "    entity_names = []\n",
    "\n",
    "    if hasattr(t, 'label') and t.label:\n",
    "        if t.label() == 'NE':\n",
    "            entity_names.append(' '.join([child[0] for child in t]))\n",
    "        else:\n",
    "            for child in t:\n",
    "                entity_names.extend(extract_entity_names(child))\n",
    "\n",
    "    return entity_names\n",
    "\n",
    "entity_names = []\n",
    "for tree in chunked_sentences:\n",
    "    #Print results per sentence\n",
    "    print(extract_entity_names(tree))\n",
    "\n",
    "    entity_names.extend(extract_entity_names(tree))\n",
    "\n",
    "# Print all entity names\n",
    "print(entity_names)\n",
    "\n",
    "# Print unique entity names\n",
    "print(set(entity_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
